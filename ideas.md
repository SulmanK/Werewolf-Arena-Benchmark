SciCode	Scientist‑curated code problems across sciences.	SciCode: A Research Coding Benchmark Curated by Scientists	https://arxiv.org/abs/2407.13168
Online–Mind2Web	Online web‑agent evaluation with 300 live tasks.	An Illusion of Progress? Assessing the Current State of LLM Web Agents (Online‑Mind2Web)	https://arxiv.org/abs/2504.01382
CORE–Bench	Agents reproduce results from real papers.	CORE‑Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark	https://arxiv.org/abs/2409.11363
GAIA	Real‑world, tool‑using assistant tasks.	GAIA: A Benchmark for General AI Assistants	https://arxiv.org/abs/2311.12983
τ–bench	Simulated user–agent dialogues with tools.	τ‑bench: A Benchmark for Tool‑Agent‑User Interaction in Real‑World Domains	https://arxiv.org/abs/2406.12045
SWE–bench (verified)	Human‑validated subset for real GitHub issues.	Introducing SWE‑bench Verified	https://openai.com/index/introducing-swe-bench-verified/
USACO benchmark	USACO problems with tests for code agents.	Can Language Models Solve Olympiad Programming?	https://arxiv.org/abs/2404.10952
Agent Battle Royale	Create a shared virtual environment for agents to "kill" each other and see who survives.		https://x.com/SIGKITTEN/status/1937950811910234377
WebShop	Simulated e‑commerce web tasks for agents.	WebShop: Towards Scalable Real‑World Web Interaction with Grounded Language Agents	https://arxiv.org/abs/2207.01206
AppWorld	Controllable multi‑app world for coding agents.	AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents	https://arxiv.org/abs/2407.18901
PersonaGym	Dynamic evaluation of persona‑following agents.	PersonaGym: Evaluating Persona Agents and LLMs	https://arxiv.org/abs/2407.18416
VERINA	Lean tasks for code+spec+proof generation.	VERINA: Benchmarking Verifiable Code Generation	https://arxiv.org/abs/2505.23135
TerminalBench	Command‑line tasks to test terminal mastery.	terminal‑bench: a benchmark for ai agents in terminal environments	https://www.tbench.ai/
Smart Contract Exploit	Agentic exploitation of real vulnerable contracts.	AI Agent Smart Contract Exploit Generation	https://arxiv.org/abs/2507.05558
BrowserGym	Unified environment for benchmarking web agents.	The BrowserGym Ecosystem for Web Agent Research	https://arxiv.org/abs/2412.05467
TheAgentCompany	Digital‑worker tasks: browse, code, run tools.	Benchmarking LLM Agents on Consequential Real World Tasks (TheAgentCompany)	https://arxiv.org/abs/2412.14161
OpenAgentSafety	Safety evals with real tools and tasks.	OpenAgentSafety: A Comprehensive Framework for Evaluating Real‑World AI Agent Safety	https://arxiv.org/abs/2507.06134
Werewolf Game	Social‑deduction game to test deception/reasoning.	Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction	https://arxiv.org/abs/2407.13943
Minecraft Gaming	Competitive Minecraft benchmark.	MCU: An Evaluation Framework for Open-Ended Game Agents	https://arxiv.org/abs/2310.08367
ALFWorld	Text‑to‑embodied tasks in aligned worlds.	ALFWorld: Aligning Text and Embodied Environments for Interactive Learning	https://arxiv.org/abs/2010.03768
OSWorld	Real OS desktop/web tasks across apps.	OSWorld: Benchmarking Multimodal Agents for Open‑Ended Tasks in Real Computer Environments	https://arxiv.org/abs/2404.07972
CRMArena	Business CRM tasks for enterprise agents.	CRMArena: Understanding the Capacity of LLM Agents to Act in Real‑World Professional Scenarios	https://arxiv.org/abs/2411.02305
DoomArena	Plug‑in security testing for agent frameworks.	DoomArena: A Framework for Testing AI Agents Against Security Threats	https://arxiv.org/abs/2504.14064
WASP	Prompt‑injection stress‑tests for web agents.	WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks	https://arxiv.org/abs/2504.18575
CyberGym	Security eval using real OSS‑Fuzz CVEs.	CyberGym: Evaluating AI Agents’ Cybersecurity Capabilities with Real‑World Vulnerabilities at Scale	https://arxiv.org/abs/2506.02548
MedAgentBench	Agent tasks in virtual EHR workflows.	MedAgentBench: A Realistic Virtual EHR Environment for Evaluating LLM Agents in Healthcare	https://arxiv.org/abs/2501.14654
LegalAgentBench	Chinese‑law agent tasks with tools.	LegalAgentBench: Evaluating LLM Agents in Legal Domain	https://arxiv.org/abs/2412.17259
Finance Agent Benchmark	SEC‑filing research tasks with tools.	Finance Agent Benchmark: Benchmarking LLMs on Real‑world Financial Research Tasks	https://arxiv.org/abs/2508.00828
CRMArena–Pro	Expanded enterprise CRM tasks and personas.	CRMArena‑Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions	https://arxiv.org/abs/2505.18878
Spider 2.0	Enterprise‑grade text‑to‑SQL workflows.	Spider 2.0: Evaluating Language Models on Real‑World Enterprise Text‑to‑SQL Workflows	https://arxiv.org/abs/2411.07763
tau2–bench	Dual‑control user+agent tool use eval.	τ²‑Bench: Evaluating Conversational Agents in a Dual‑Control Setting	https://arxiv.org/abs/2506.07982